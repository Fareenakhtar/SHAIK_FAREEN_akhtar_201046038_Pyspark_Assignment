{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"RDD ASSIGNMENT\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create RDDs in three different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Creating RDD using Parallelize method\n",
    "\n",
    "rdd_par = spark.sparkContext.parallelize([\"Hello world\", \"Hope you are not fed up with ABD class\", \"ello\"])\n",
    "#rdd_par.collect() #to see the record by giving the index number\n",
    "rdd_par.count()   #to count the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world', 'Hope you are not fed up with ABD class']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.Creating RDD using Transformations\n",
    "\n",
    "rdd_trans = rdd_par.filter(lambda word:word.startswith('H')) #creating rdd\n",
    "rdd_trans.collect()    #executing rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data analytics is the process of collecting, organizing and analyzing large sets of data (called Big Data) to discover patterns and other useful information. ',\n",
       " 'Big Data analytics can help organizations to better understand the information contained within the data.',\n",
       " 'It will also help identify the data that is most important to the business and future business decisions. ',\n",
       " 'Analysts working with Big Data typically want the knowledge that comes from analyzing the data.',\n",
       " '']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Creating RDD using Data sources\n",
    "\n",
    "rdd_ds = spark.sparkContext.textFile('E:/BIG_DATA_AND_DATA_ANALYTICS/SEM_1/ARCHITECTURE_OF_BIG_DATA_SYSTEM/input.txt')\n",
    "rdd_ds.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read a text file and count the number of words in the file using RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ds.flatMap(lambda word:word.split(' ')) #splits into words\n",
    "rdd_ds.flatMap(lambda word:word.split(' ')).count() # gives the number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write a program to find the word frequency in a given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('analytics', 2),\n",
       " ('is', 2),\n",
       " ('process', 1),\n",
       " ('of', 2),\n",
       " ('organizing', 1),\n",
       " ('large', 1),\n",
       " ('sets', 1),\n",
       " ('(called', 1),\n",
       " ('Data)', 1),\n",
       " ('discover', 1),\n",
       " ('other', 1),\n",
       " ('useful', 1),\n",
       " ('information.', 1),\n",
       " ('', 3),\n",
       " ('help', 2),\n",
       " ('organizations', 1),\n",
       " ('understand', 1),\n",
       " ('data.', 2),\n",
       " ('It', 1),\n",
       " ('identify', 1),\n",
       " ('business', 2),\n",
       " ('decisions.', 1),\n",
       " ('working', 1),\n",
       " ('typically', 1),\n",
       " ('knowledge', 1),\n",
       " ('Big', 4),\n",
       " ('Data', 3),\n",
       " ('the', 7),\n",
       " ('collecting,', 1),\n",
       " ('and', 3),\n",
       " ('analyzing', 2),\n",
       " ('data', 2),\n",
       " ('to', 3),\n",
       " ('patterns', 1),\n",
       " ('can', 1),\n",
       " ('better', 1),\n",
       " ('information', 1),\n",
       " ('contained', 1),\n",
       " ('within', 1),\n",
       " ('will', 1),\n",
       " ('also', 1),\n",
       " ('that', 2),\n",
       " ('most', 1),\n",
       " ('important', 1),\n",
       " ('future', 1),\n",
       " ('Analysts', 1),\n",
       " ('with', 1),\n",
       " ('want', 1),\n",
       " ('comes', 1),\n",
       " ('from', 1)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rdd = rdd_ds.flatMap(lambda word:word.split(' '))\n",
    "freq_words = word_rdd.map(lambda word: (word,1)) # create a tuple for each element\n",
    "freq_words.reduceByKey(lambda a,b : a+b)    # check how many times each word appear\n",
    "#freq_words.reduceByKey(lambda a,b : a+b).count() # counts the number of words\n",
    "freq_words.reduceByKey(lambda a,b : a+b).collect() # execution. number of times each word appear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write a program to convert all words in a file to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BIG DATA ANALYTICS IS THE PROCESS OF COLLECTING, ORGANIZING AND ANALYZING LARGE SETS OF DATA (CALLED BIG DATA) TO DISCOVER PATTERNS AND OTHER USEFUL INFORMATION. ',\n",
       " 'BIG DATA ANALYTICS CAN HELP ORGANIZATIONS TO BETTER UNDERSTAND THE INFORMATION CONTAINED WITHIN THE DATA.',\n",
       " 'IT WILL ALSO HELP IDENTIFY THE DATA THAT IS MOST IMPORTANT TO THE BUSINESS AND FUTURE BUSINESS DECISIONS. ',\n",
       " 'ANALYSTS WORKING WITH BIG DATA TYPICALLY WANT THE KNOWLEDGE THAT COMES FROM ANALYZING THE DATA.',\n",
       " '']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ds.map(lambda c:c.upper()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write a program to convert all words in a file to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big data analytics is the process of collecting, organizing and analyzing large sets of data (called big data) to discover patterns and other useful information. ',\n",
       " 'big data analytics can help organizations to better understand the information contained within the data.',\n",
       " 'it will also help identify the data that is most important to the business and future business decisions. ',\n",
       " 'analysts working with big data typically want the knowledge that comes from analyzing the data.',\n",
       " '']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ds.map(lambda c:c.lower()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Write a program to capitalize first letter of each words in file (use string capitalize() method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Big Data Analytics Is The Process Of Collecting, Organizing And Analyzing Large Sets Of Data (called Big Data) To Discover Patterns And Other Useful Information.  Big Data Analytics Can Help Organizations To Better Understand The Information Contained Within The Data. It Will Also Help Identify The Data That Is Most Important To The Business And Future Business Decisions.  Analysts Working With Big Data Typically Want The Knowledge That Comes From Analyzing The Data. '"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_UpperCase = rdd_ds.flatMap(lambda a:a.split(' ')).map(lambda c:c.capitalize()).collect()\n",
    "\" \".join(first_UpperCase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Find the longest length of word from given set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  13\n",
      "Word:  organizations\n"
     ]
    }
   ],
   "source": [
    "LongWord=rdd_ds.flatMap(lambda x:x.split(' '))\n",
    "long=LongWord.map(lambda word:(len(word),word)).max()\n",
    "print(\"Length: \",long[0])\n",
    "print(\"Word: \",long[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Map the Registration numbers to corresponding branch. \n",
    "### 6000 series BDA, \n",
    "### 9000 series HDA, \n",
    "### 1000 series ML, \n",
    "### 2000 series VLSI, \n",
    "### 3000 series ES, \n",
    "### 4000 series MSc, \n",
    "### 5000 series CC. \n",
    "### Given registration number, generate a key-value pair of Registration Number and Corresponding Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9001, 'HDA'), (9003, 'HDA'), (4002, 'MSc'), (5008, 'CC'), (9004, 'HDA'), (8005, 'HDA'), (3006, 'ES'), (1008, 'ML'), (4008, 'MSc'), (6002, 'BDA'), (7004, 'HDA'), (8005, 'HDA'), (6007, 'BDA')]\n"
     ]
    }
   ],
   "source": [
    "reg_no = [9001,9003,4002,5008,9004,8005,3006,1008,4008,6002,7004,8005,6007]\n",
    "rdd_keyValue=spark.sparkContext.parallelize(reg_no,2)\n",
    "gen=rdd_keyValue.map(lambda reg_no:(reg_no,'ML') if reg_no>1000 and reg_no<2000 \n",
    "        else (reg_no,'VLSI') if reg_no>2000 and reg_no<3000\n",
    "        else (reg_no,'ES') if reg_no>3000 and reg_no<4000\n",
    "        else (reg_no,'MSc') if reg_no>4000 and reg_no<5000\n",
    "        else (reg_no,'CC') if reg_no>5000 and reg_no<6000\n",
    "        else (reg_no,'BDA') if reg_no>6000 and reg_no<7000\n",
    "        else (reg_no,'HDA'))\n",
    "gen_value = gen.collect()\n",
    "print(gen_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Text file contain numbers. Numbers are separated by one white space. There is no order to store the numbers. One line may contain one or more numbers. Find the maximum, minimum, sum and mean of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value:  33.0\n",
      "Min value:  0.0\n",
      "Sum value:  61.0\n",
      "Mean value:  8.714285714285715\n"
     ]
    }
   ],
   "source": [
    "rdd_num = spark.sparkContext.textFile(\"numbers.txt\")\n",
    "num= rdd_num.flatMap(lambda x : x.split(' '))\n",
    "num_rdd=num.map(lambda x:float(x))\n",
    "\n",
    "max_num=num_rdd.max()\n",
    "print(\"Max value: \",max_num)\n",
    "\n",
    "min_num=num_rdd.min()\n",
    "print(\"Min value: \",min_num)\n",
    "\n",
    "sum_num=num_rdd.sum()\n",
    "print(\"Sum value: \",sum_num)\n",
    "\n",
    "mean=num_rdd.mean()\n",
    "print(\"Mean value: \",mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, Phone, email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc. Compress the citizen.txt file by changing full state name to state code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Fareen', '13-08-1998', '999999999', 'fareen@gmail.com', 'KA'],\n",
       " ['Namrutha', '08-09-1998', '9898989898', 'nam@gmail.com', 'AP'],\n",
       " ['Madhuri', '8-08-1997', '9797079797', 'madu@gmail.com', 'DL'],\n",
       " ['James', '12-12-1997', '9090909090', 'james@gmail.com', 'UP']]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Citizen_rdd = spark.sparkContext.textFile(\"citizen.txt\")\n",
    "State_rdd = spark.sparkContext.textFile(\"mappingStateCode.txt\")\n",
    "\n",
    "details_rdd = Citizen_details.map(lambda x:x.split(\",\")).collect()\n",
    "Statecode_rdd= State_rdd.map(lambda y:y.split(\",\")).collect()\n",
    "\n",
    "for i in range(len(details_rdd)):\n",
    "    for j in range(len(Statecode_rdd)):  \n",
    "        if details_rdd[i][4]==Statecode_rdd[j][0]:\n",
    "            details_rdd[i][4]=Statecode_rdd[j][1]\n",
    "details_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'karnataka': 'KA', 'AndraPradesh': 'AP', 'Delhi': 'DL', 'UttarPradesh': 'UP'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = spark.sparkContext.textFile('mappingStateCode.txt')\n",
    "stateKey = state.map(lambda word: (word.split(\",\")[0], word.split(\",\")[1]))\n",
    "\n",
    "Statecode_dict = {}\n",
    "for val in stateKey.collect():\n",
    "    Statecode_dict[val[0]] = val[1]\n",
    "Statecode_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fareen 13-08-1998 999999999 fareen@gmail.com KA',\n",
       " 'Namrutha 08-09-1998 9898989898 nam@gmail.com AP',\n",
       " 'Madhuri 8-08-1997 9797079797 madu@gmail.com DL',\n",
       " 'James 12-12-1997 9090909090 james@gmail.com UP']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapData = spark.sparkContext.broadcast(Statecode_dict)\n",
    "def compress(state,codes):\n",
    "    dataSplit = state.split(\",\")\n",
    "    dataSplit[4] = codes.value.get(dataSplit[4])\n",
    "    data_new = ' '\n",
    "    data_new = data_new.join(dataSplit)\n",
    "    return data_new\n",
    "Citizen_rdd.map(lambda data:compress(data, mapData)).collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
